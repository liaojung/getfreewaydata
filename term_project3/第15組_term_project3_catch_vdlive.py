# -*- coding: utf-8 -*-
"""第15組_Term Project3_catch_VDlive.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jPuR5u74KzDyeXJBUPx0nGkRwCnhTG1u
"""

#原本在colab執行，更改28、59、102行的存取位置就能執行(關於下載、存取)
#38-46行能選地點，54、55行能選時間(資料處理的) 這邊舉例所以我沒有寫所有時間，但可以直接改

import numpy as np
import pandas as pd
import os
import gzip
import shutil
import requests
from datetime import datetime, timedelta
import xml.etree.ElementTree as ET

start_time = datetime.strptime('1601', '%H%M')
end_time = datetime.strptime('2000', '%H%M')
time_interval = timedelta(minutes=1)

while start_time <= end_time:
  url = 'https://tisvcloud.freeway.gov.tw/history/motc20/VD/20231222/VDLive_'+start_time.strftime('%H%M')+'.xml.gz'
  path = "C:/Users/yingj/nycu/11201/巨集/3 陳彥佑/term_project3/data/"
  gz_file = "VDlive_1222_"+start_time.strftime('%H%M')+".xml.gz"
  uncompressed_file_name = "VDlive_1222_"+start_time.strftime('%H%M')+".xml"
  response = requests.get(url)
  with open(os.path.join(path, gz_file), 'wb') as file:
    file.write(response.content)
  with gzip.open(os.path.join(path, gz_file), 'rb') as file, open(os.path.join(path, uncompressed_file_name), 'wb') as uncompressed:
    shutil.copyfileobj(file, uncompressed)
  start_time += time_interval

#23圓山_25台北_27三重_32五股
#圓山_01F0248S_台北_01F0264S_三重_01F0293S_五股(南下)
# 下交流道
#VD = ['VD-N1-S-23-O-NE-1-圓山','VD-N1-S-25-O-NW-1A-台北','VD-N1-S-25-O-NE-1B-台北','VD-N1-S-25-I-ES-22-台北','VD-N1-S-27-O-NW-1-三重','VD-N1-S-27-O-NE-22-三重']
# 上交流道
#VD = ['VD-N1-S-23-I-WS-31-圓山','VD-N1-S-23-I-WS-32-圓山','VD-N1-S-25-I-WS-21-台北','VD-N1-S-27-I-ES-1-三重']
# main
VD = ['VD-N1-S-23.280-N-LOOP','VD-N1-S-23.900-M-LOOP','VD-N1-S-24.400-M-RS','VD-N1-S-25.850-M-LOOP','VD-N1-S-26.350-M-RS','VD-N1-S-28.840-M-RS']
VD_selected = VD[4]

columns = ['speed','occupancy']
for i in ['S', 'L', 'T']:
  for j in ['vol','speed']:
    columns.append(i+"_"+j)
data_sets = pd.DataFrame(columns = columns)

start_time = datetime.strptime('1601', '%H%M')
end_time = datetime.strptime('1603', '%H%M')
time_interval = timedelta(minutes=1)
time_index = start_time
while time_index <= end_time:
  file = 'C:/Users/yingj/nycu/11201/巨集/3 陳彥佑/term_project3/data/VDlive_1222_'+time_index.strftime('%H%M')+'.xml'
  namespace = {"ns": "http://traffic.transportdata.tw/standard/traffic/schema/"}
  tree = ET.parse(file)

  vd_lives = tree.findall("./ns:VDLives/ns:VDLive", namespaces = namespace)
  data_lane = np.zeros((4,2))
  data_vehicles = np.zeros((4,3,2))
  carType = {"S":0, "L":1, "T":2}

  for vd_live in vd_lives:
    vd_id = vd_live.find("./ns:VDID", namespaces = namespace).text
    if vd_id == VD_selected:
        lanes = vd_live.findall("./ns:LinkFlows/ns:LinkFlow/ns:Lanes/ns:Lane", namespace)
        for lane in lanes:
            lane_id = int(lane.find("ns:LaneID", namespaces=namespace).text)
            speed_by_lane = int(lane.find("ns:Speed", namespaces=namespace).text)
            occupancy = int(lane.find("ns:Occupancy" , namespaces=namespace).text)

            data_lane[lane_id][0] = speed_by_lane
            data_lane[lane_id][1] = occupancy

            vehicles = lane.findall("ns:Vehicles/ns:Vehicle" , namespaces=namespace)

            for vehicle in vehicles:
                vehicletype = vehicle.find("ns:VehicleType" , namespaces=namespace).text
                volume = int(vehicle.find("ns:Volume" , namespaces=namespace).text)
                speed = int(vehicle.find("ns:Speed" , namespaces=namespace).text)
                data_vehicles[lane_id][carType[vehicletype]][0] = volume
                data_vehicles[lane_id][carType[vehicletype]][1] = speed
        data_vehicles = data_vehicles.reshape(4,6)
        #print(data_vehicles)
        #print(data_lane)

  data_lane = np.concatenate((data_lane, data_vehicles),axis = 1)
  index_dataframe = pd.DataFrame(['lane0','lane1','lane2','lane3'], columns = ['lanes'])
  index_dataframe['time'] = time_index.strftime('%H:%M')
  index = pd.MultiIndex.from_frame(index_dataframe)
  data = pd.DataFrame(data_lane, index = index, columns = columns).reset_index()
  data_sets = pd.concat([data_sets, data], ignore_index=True)
  time_index += time_interval
data_sets = data_sets.set_index(keys = ['lanes','time']).unstack(level = 0)
print(data_sets)

data_sets.to_csv('C:/Users/yingj/nycu/11201/巨集/3 陳彥佑/term_project3/VDlive_1222_'+ VD_selected[6:13]+"_" +start_time.strftime('%H%M') + "_" + end_time.strftime('%H%M')+'.csv')